{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, im going through two boosting methods, in particular:\n",
    "\n",
    "Gradient Boosting \n",
    "\n",
    "\n",
    "Adaptive Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_data(filename):\n",
    "\n",
    "    data = pd.read_csv(filename, sep = '\\t', header = None)\n",
    "\n",
    "    data.columns = ['Frequency','Angle of Attack','Chord length','Free stream velocity','Suction','Pressure']\n",
    "\n",
    "    return data\n",
    "\n",
    "filename = 'airfoil_self_noise.dat'\n",
    "\n",
    "df = get_data(filename)\n",
    "\n",
    "for _ in range(10):\n",
    "    df.sample(frac = 1)\n",
    "\n",
    "Y = df['Pressure']\n",
    "X = df[['Frequency','Angle of Attack', 'Chord length','Free stream velocity','Suction']]\n",
    "\n",
    "split = 0.8\n",
    "split*=len(X)\n",
    "split = int(split)\n",
    "\n",
    "X_train ,X_val = X[:split], X[split:]\n",
    "y_train, y_val = Y[:split], Y[split:]\n",
    "\n",
    "\n",
    "N, p= X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.42246255104156"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what would vanilla linear regression give\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "mean_squared_error(y_pred, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let us try some gradient boosting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoosting:\n",
    "    def __init__(self, loss_function, dloss_function):\n",
    "        self.loss = loss_function\n",
    "        self.dloss = dloss_function\n",
    "        self.models = []\n",
    "        self.gammas = []\n",
    "        \n",
    "    def setUp(self, X_train, y_train):\n",
    "        self.X = X_train\n",
    "        self.N, self.p = self.X.shape\n",
    "        self.y = y_train\n",
    "        \n",
    "    def _find_gamma(self, gamma, F_init, h_model):\n",
    "        return self.loss(self.y, F_init + gamma*h_model.predict(self.X))\n",
    "                         \n",
    "    def _find_h_model(self, model, F_init):\n",
    "        \"\"\"\n",
    "        Assumes to have a model.fit API call.\n",
    "        \"\"\"\n",
    "        #F_init = np.mean(self.y)\n",
    "        L_diff = -self.dloss(self.y,F_init)\n",
    "        model.fit(X_train, L_diff)\n",
    "        return model\n",
    "    \n",
    "    def _minimise_for_gamma(self, gamma_0, F_init, h_model, opt_verbose = False):\n",
    "        def get_gamma(gamma):\n",
    "            return self._find_gamma(gamma, F_init, h_model)\n",
    "        \n",
    "        res = minimize(get_gamma, gamma_0, method='nelder-mead',\n",
    "                       options={'xatol': 1e-8, 'disp': opt_verbose})\n",
    "\n",
    "        gamma_next = res['x']\n",
    "        return gamma_next\n",
    "        \n",
    "        #F_next = F_init + gamma_next*hm.predict(X_train)\n",
    "        \n",
    "    def _get_F_next(self, model, F_init, gamma_0,opt_verbose = False, print_current_loss = False):\n",
    "        h_model = self._find_h_model(model, F_init)\n",
    "\n",
    "        gamma_next = self._minimise_for_gamma(gamma_0, F_init, h_model, opt_verbose)\n",
    "\n",
    "        F_next = F_init + gamma_next*h_model.predict(self.X)\n",
    "        \n",
    "        if print_current_loss:\n",
    "            print(self.loss(F_next, self.y))\n",
    "        return F_next, gamma_next\n",
    "    \n",
    "    def run_iteration(self, F_init, model, gamma_0, print_every = 50):\n",
    "\n",
    "        gamma_0 = 200\n",
    "        if print_every and print_every%50 == 0:\n",
    "            F_next,gamma_next = gb._get_F_next(model, F_init, gamma_0, False, True)\n",
    "        else:\n",
    "            F_next,gamma_next = gb._get_F_next(model, F_init, gamma_0, False, False)\n",
    "            \n",
    "        self.models.append(model)\n",
    "        self.gammas.append(gamma_next)\n",
    "        F_init = F_next\n",
    "        \n",
    "        \n",
    "        return F_init\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, gamma_0 = 0.5, models = None, max_iteration = 10, print_every = None):\n",
    "        self.setUp(X, y)\n",
    "        F_init = np.mean(self.y)\n",
    "        self.F0 = F_init\n",
    "        if models is None:\n",
    "            for M in range(0,max_iteration):\n",
    "                model = LinearRegression()\n",
    "                F_init = gb.run_iteration(F_init, model, gamma_0, print_every)\n",
    "            \n",
    "            \n",
    "    def predict(self, X):\n",
    "        M = len(self.gammas)\n",
    "        y_pred = np.sum(np.array([self.gammas[i]*self.models[i].predict(X) for i in range(0,M)]),0) + self.F0\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.42246255217496"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss(x,y):\n",
    "    diff = x-y\n",
    "    diff = diff**2\n",
    "    return np.mean(diff)\n",
    "\n",
    "def dloss(x,y):\n",
    "    diff = (y - x)/len(x)\n",
    "    return diff\n",
    "\n",
    "gb = GradientBoosting(loss, dloss)\n",
    "gb.setUp(X_train, y_train)\n",
    "\n",
    "F_init = np.mean(gb.y)\n",
    "model = LinearRegression()\n",
    "gamma_0 = 0.0\n",
    "gb.fit(X_train, y_train)\n",
    "loss(gb.predict(X_val), y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.02107903751532"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tree algorithm?\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "mean_squared_error(model.predict(X_val),y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from tqdm import tqdm_notebook\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "class GradientBoostingTreeRegressor:\n",
    "    def __init__(self, max_depth, loss_function,  max_leaf_nodes = None):\n",
    "        self.max_depth = max_depth\n",
    "        self.max_leaf_nodes = max_leaf_nodes\n",
    "        self.loss = loss_function\n",
    "        \n",
    "    def _get_gamma_general(self, gamma, idx, F_prev):\n",
    "        return self.loss(self.y[idx],F_prev[idx] + gamma)\n",
    "\n",
    "    def _get_all_gammas(self,regions, all_leaves_indices, F_prev, gamma_0 = 0):\n",
    "        gamma_per_region = {}\n",
    "        for region in tqdm_notebook(regions, leave = True):\n",
    "            idx, = np.where(all_leaves_indices== region)\n",
    "\n",
    "            def get_gamma(gamma):\n",
    "                return self._get_gamma_general(gamma, idx, F_prev)\n",
    "\n",
    "            res = minimize(get_gamma, gamma_0, method='nelder-mead',  options={'xatol': 1e-8, 'disp': False})\n",
    "\n",
    "            gamma_per_region[region] = res['x'][0]\n",
    "\n",
    "        return gamma_per_region\n",
    "    \n",
    "    def setUp(self ,X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def _train_F0_model(self):\n",
    "        model_0 = DecisionTreeRegressor(max_depth = self.max_depth, max_leaf_nodes = self.max_leaf_nodes)\n",
    "        model_0.fit(self.X, self.y)\n",
    "        self.model_0 = model_0\n",
    "        return model_0\n",
    "        \n",
    "    def fit(self, X, y, max_iterations = 2):\n",
    "        self.setUp(X,y)\n",
    "        self.gamma_per_m = {}\n",
    "        model_0 = self._train_F0_model()\n",
    "        \n",
    "        all_leaves = model_0.apply(self.X)\n",
    "        regions = np.unique(all_leaves)\n",
    "        J = len(regions)\n",
    "\n",
    "        F_prev = np.array([np.mean(y_train)]*len(y_train))\n",
    "\n",
    "        for i in range(0,max_iterations):\n",
    "            print(self.loss(F_prev, y_train))\n",
    "\n",
    "\n",
    "            gamma_per_region = self._get_all_gammas(regions,all_leaves, F_prev )\n",
    "            self.gamma_per_m[i] = gamma_per_region\n",
    "            gamma_per_region_function = lambda x: gamma_per_region[x]\n",
    "\n",
    "            F_prev = F_prev + list(map(gamma_per_region_function,model_0.apply(X_train)))\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \n",
    "        def get_gammas(i):\n",
    "            return lambda x: self.gamma_per_m[i][x]\n",
    "        \n",
    "        res = np.mean(y_train)\n",
    "        for k in self.gamma_per_m.keys():\n",
    "            res += np.array(list(map(get_gammas(k),self.model_0.apply(self.X))))\n",
    "        \n",
    "        return res\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.060230917761174\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9beb127dba8c4c73815bb28697c3a03b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=116.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7.691384372374305\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851b7cccb07b4a01bc23acf82fc86592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=116.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingTreeRegressor(max_depth = 7, loss_function = mean_squared_error)\n",
    "gb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.691384372374305"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff = gb.predict(X_train) - y_train\n",
    "np.mean(diff**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.492315679317933"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see how this compares with XGBoost.\n",
    "\n",
    "import xgboost\n",
    "\n",
    "xgb = xgboost.XGBRegressor(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75,\n",
    "                           colsample_bytree=1, max_depth=7)\n",
    "\n",
    "xgb.fit(X_train,y_train)\n",
    "\n",
    "y_pred = xgb.predict(X_val)\n",
    "\n",
    "loss(y_pred, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('ml_env': conda)",
   "language": "python",
   "name": "python36964bitmlenvconda9f54039d931e4029adcffd4ea832f0f0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
